{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c811944-f10a-496a-a239-a0686dd51394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "632378ea-3abd-455b-bffb-222a15c1f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    encoded = np.zeros((y.size, num_classes))\n",
    "    encoded[np.arange(y.size), y] = 1\n",
    "    return encoded\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_tanh(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def d_relu(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def d_mse(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "821bd357-0df7-4e6f-a203-d9e92e25b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size): \n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01 #weight matrix from input to hidden layer\n",
    "        self.b1 = np.zeros((1, hidden_size)) #bias vector for hidden layer\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01 #weight matrix from hidden to output layer \n",
    "        self.b2 = np.zeros((1, output_size)) #bias vector for output layer\n",
    "\n",
    "      \n",
    "    def __init__(self, input_size, hidden_size, output_size, activation, d_activation):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "        self.activation = activation              # hidden layer activation\n",
    "        self.d_activation = d_activation         \n",
    "\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        self.Z1 = X @ self.W1 + self.b1\n",
    "        self.A1 = self.activation(self.Z1)       \n",
    "        self.Z2 = self.A1 @ self.W2 + self.b2\n",
    "        self.A2 = self.activation(self.Z2)              \n",
    "        return self.A2\n",
    "\n",
    "        \n",
    "\n",
    "    def backpropagate(self, X, y):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        dZ2 = 2 * (self.A2 - y) * self.d_activation(self.Z2)\n",
    "        dW2 = self.A1.T @ dZ2\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "        dZ1 = (dZ2 @ self.W2.T) * self.d_activation(self.Z1)\n",
    "        dW1 = X.T @ dZ1\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "        return dW1 / m, db1 / m, dW2 / m, db2 / m\n",
    "\n",
    "        \n",
    "    \n",
    "    def train(self, X, y, epochs, lr):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.feedforward(X) \n",
    "            loss = mse(y, output)\n",
    "            dW1, db1, dW2, db2 = self.backpropagate(X, y)\n",
    "\n",
    "            self.W1 -= lr * dW1\n",
    "            self.b1 -= lr * db1\n",
    "            self.W2 -= lr * dW2\n",
    "            self.b2 -= lr * db2\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "                \n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.feedforward(X) #array-row corresponds to predicted possibilities for each class\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(np.argmax(y_true, axis=1), y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26b310b0-d1ef-4f82-a9a3-5519fad4ea42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with sigmoid activation:\n",
      "Epoch 0, Loss: 0.2472\n",
      "Epoch 100, Loss: 0.0896\n",
      "Epoch 200, Loss: 0.0830\n",
      "Epoch 300, Loss: 0.0536\n",
      "Epoch 400, Loss: 0.0376\n",
      "Epoch 500, Loss: 0.0266\n",
      "Epoch 600, Loss: 0.0198\n",
      "Epoch 700, Loss: 0.0145\n",
      "Epoch 800, Loss: 0.0116\n",
      "Epoch 900, Loss: 0.0097\n",
      "Sigmoid Accuracy: 95.83%\n",
      "\n",
      "Training with tanh activation:\n",
      "Epoch 0, Loss: 0.1002\n",
      "Epoch 100, Loss: 0.0318\n",
      "Epoch 200, Loss: 0.0289\n",
      "Epoch 300, Loss: 0.0264\n",
      "Epoch 400, Loss: 0.0229\n",
      "Epoch 500, Loss: 0.0232\n",
      "Epoch 600, Loss: 0.0215\n",
      "Epoch 700, Loss: 0.0223\n",
      "Epoch 800, Loss: 0.0210\n",
      "Epoch 900, Loss: 0.0199\n",
      "Tanh Accuracy: 96.67%\n",
      "\n",
      "Training with relu activation:\n",
      "Epoch 0, Loss: 0.0998\n",
      "Epoch 100, Loss: 0.0051\n",
      "Epoch 200, Loss: 0.0035\n",
      "Epoch 300, Loss: 0.0028\n",
      "Epoch 400, Loss: 0.0022\n",
      "Epoch 500, Loss: 0.0017\n",
      "Epoch 600, Loss: 0.0014\n",
      "Epoch 700, Loss: 0.0012\n",
      "Epoch 800, Loss: 0.0012\n",
      "Epoch 900, Loss: 0.0011\n",
      "Relu Accuracy: 97.50%\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data #features\n",
    "y = digits.target #actual digits corresponding to each image\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "y_encoded = one_hot_encode(y, 10)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# training NN \n",
    "#nn = NeuralNetwork(64, 30, 10)\n",
    "#nn.train(X_train, y_train, epochs=1000, lr=0.5)\n",
    "\n",
    "#accuracy = nn.evaluate(X_test, y_test)\n",
    "#print(f\"Test Accuracy (Sigmoid): {accuracy * 100:.2f}%\")\n",
    "\n",
    "activations = {\n",
    "    \"sigmoid\": (sigmoid, d_sigmoid),\n",
    "    \"tanh\": (tanh, d_tanh),\n",
    "    \"relu\": (relu, d_relu)\n",
    "}\n",
    "\n",
    "for name, (act_fn, d_act_fn) in activations.items():\n",
    "    print(f\"\\nTraining with {name} activation:\")\n",
    "    nn = NeuralNetwork(64, 30, 10, act_fn, d_act_fn)\n",
    "    nn.train(X_train, y_train, epochs=1000, lr=0.5)\n",
    "    accuracy = nn.evaluate(X_test, y_test)\n",
    "    print(f\"{name.capitalize()} Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08596d0-437c-4cd7-bbf2-70539ed4eced",
   "metadata": {},
   "source": [
    "\n",
    "The optimal learning rate was found to be 0.5, especially for the sigmoid activation function, which showed a significant improvement in performance in this case. When using a smaller learning rate, such as 0.1, the model achieved very low accuracy (~41.67%) with sigmoid, while both tanh and ReLU maintained high accuracies (~95.8% and 97.2%, respectively).\n",
    "\n",
    "This likely happened because of how the sigmoid function tends to saturate, especially with smaller weight updates. At low learning rates, the gradients can become small (vanishing gradient problem), so the model updates very slowly, which affects learning. On the other hand, tanh (which is zero-centered) and ReLU (which avoids saturation in the positive range) are better at keeping the gradient strong enough to learn even with smaller learning rates.\n",
    "\n",
    "Increasing the learning rate to 0.5 allowed the sigmoid function to escape these slow learning regions, improving its accuracy significantly. However,the learning rate was not set too high even though the accuracy for sigmoid activation function was slightly better with higher learning rates, as this could lead to unstable training or overshooting the minimum\n",
    "\n",
    "Similarly, increasing the number of epochs to 1000 was ideal for the sigmoid activation, giving the network more time to converge and make meaningful updates. When fewer epochs were used, the sigmoid-based model struggled to achieve good performance due to its slower learning dynamics. The number of epochs was capped at 1000, even though training for longer slightly improved the accuracy for sigmoid to balance performance with training time. Since sigmoid typically learns more slowly than ReLU or tanh, giving it more epochs helps it catch up but 1000 epochs were found to be a time reasonable upper limit for consistent training across all three activations.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
