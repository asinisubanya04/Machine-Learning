{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a189c0be-67fb-4335-aadd-aa1f235bc2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 06:57:42.557272: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist # mnist.load_data()\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a32ce2ca-d942-4b4e-b5c0-114054f2b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -15, 15)))  # Clip to avoid overflow\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    # Add a small epsilon to prevent log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "    log_likelihood = -np.sum(y_true * np.log(y_pred)) / m\n",
    "    return log_likelihood\n",
    "\n",
    "# One-hot encoding\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    return np.eye(num_classes)[y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec13b14-59f2-4099-9fce-b0dad19bd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, num_filters, kernel_size, stride=1):\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.filters = None  # Will be initialized after input is known\n",
    "        self.biases = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "     \n",
    "        #input_data shape: (batch_size, channels, height, width)\n",
    "     \n",
    "        self.input = input_data\n",
    "        batch_size, channels, height, width = input_data.shape\n",
    "\n",
    "        if self.filters is None:\n",
    "            # Filters: (num_filters, channels, kernel_size, kernel_size)\n",
    "            self.filters = np.random.randn(self.num_filters, channels, self.kernel_size, self.kernel_size) * 0.1\n",
    "            self.biases = np.zeros(self.num_filters)\n",
    "\n",
    "        output_height = (height - self.kernel_size) // self.stride + 1\n",
    "        output_width = (width - self.kernel_size) // self.stride + 1\n",
    "        output = np.zeros((batch_size, self.num_filters, output_height, output_width))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for f in range(self.num_filters):\n",
    "                for h in range(output_height):\n",
    "                    for w in range(output_width):\n",
    "                        h_start = h * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = w * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "\n",
    "                        region = input_data[b, :, h_start:h_end, w_start:w_end]\n",
    "                        output[b, f, h, w] = np.sum(region * self.filters[f]) + self.biases[f]\n",
    "\n",
    "\n",
    "        self.output_pre_activation = output\n",
    "        self.output = sigmoid(output)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues, learning_rate=0.01):\n",
    "        batch_size, input_channels, input_height, input_width = self.input.shape\n",
    "        _, _, output_height, output_width = dvalues.shape\n",
    "\n",
    "        dinputs = np.zeros_like(self.input)\n",
    "        dfilters = np.zeros_like(self.filters)\n",
    "        dbiases = np.zeros_like(self.biases)\n",
    "\n",
    "        dactivation = dvalues * sigmoid_derivative(self.output)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for f in range(self.num_filters):\n",
    "                for h in range(output_height):\n",
    "                    for w in range(output_width):\n",
    "                        h_start = h * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = w * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "\n",
    "                        region = self.input[b, :, h_start:h_end, w_start:w_end]\n",
    "                        dfilters[f] += dactivation[b, f, h, w] * region\n",
    "                        dinputs[b, :, h_start:h_end, w_start:w_end] += dactivation[b, f, h, w] * self.filters[f]\n",
    "                        dbiases[f] += dactivation[b, f, h, w]\n",
    "\n",
    "        self.filters -= learning_rate * dfilters / batch_size\n",
    "        self.biases -= learning_rate * dbiases / batch_size\n",
    "\n",
    "        return dinputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236edda1-c261-4285-9065-94004de29d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPoolLayer:\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        #Forward pass for average pooling layer\n",
    "        #input_data shape: (batch_size, channels, height, width)\n",
    "        self.input = input_data\n",
    "        batch_size, channels, height, width = input_data.shape\n",
    "        \n",
    "        output_height = (height - self.pool_size) // self.stride + 1\n",
    "        output_width = (width - self.pool_size) // self.stride + 1\n",
    "        \n",
    "        output = np.zeros((batch_size, channels, output_height, output_width))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for h in range(output_height):\n",
    "                    for w in range(output_width):\n",
    "                        h_start = h * self.stride\n",
    "                        h_end = h_start + self.pool_size\n",
    "                        w_start = w * self.stride\n",
    "                        w_end = w_start + self.pool_size\n",
    "                        \n",
    "                        output[b, c, h, w] = np.mean(input_data[b, c, h_start:h_end, w_start:w_end])\n",
    "        \n",
    "        self.output = output\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "   \n",
    "        #Backward pass for average pooling layer\n",
    "        #dvalues shape: (batch_size, channels, output_height, output_width)\n",
    "    \n",
    "        batch_size, channels, output_height, output_width = dvalues.shape\n",
    "        dinputs = np.zeros_like(self.input)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for h in range(output_height):\n",
    "                    for w in range(output_width):\n",
    "                        h_start = h * self.stride\n",
    "                        h_end = h_start + self.pool_size\n",
    "                        w_start = w * self.stride\n",
    "                        w_end = w_start + self.pool_size\n",
    "                        \n",
    "                        # Distribute gradient equally to all positions in the pooling window\n",
    "                        avg_gradient = dvalues[b, c, h, w] / (self.pool_size * self.pool_size)\n",
    "                        dinputs[b, c, h_start:h_end, w_start:w_end] += np.ones((self.pool_size, self.pool_size)) * avg_gradient\n",
    "        \n",
    "        return dinputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc0af5a-206a-4860-b552-6e65e4f07ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "      \n",
    "        #Forward pass for flatten layer\n",
    "        #input_data shape: (batch_size, channels, height, width)\n",
    "       \n",
    "        self.input_shape = input_data.shape\n",
    "        batch_size = input_data.shape[0]\n",
    "        return input_data.reshape(batch_size, -1)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "      \n",
    "        #Backward pass for flatten layer\n",
    "        #dvalues shape: (batch_size, flattened_size)\n",
    "        \n",
    "        return dvalues.reshape(self.input_shape)\n",
    "\n",
    "class Conv1x1Layer:\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "       \n",
    "        #1x1 Convolution layer (equivalent to fully connected layer with specific structure)\n",
    "       \n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        # Initialize weights with small random values\n",
    "        self.weights = np.random.randn(input_channels, output_channels) * 0.01\n",
    "        self.biases = np.zeros(output_channels)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        \n",
    "        #Forward pass for 1x1 convolution layer\n",
    "        #input_data shape: (batch_size, flattened_size)\n",
    "        \n",
    "        self.input = input_data\n",
    "        # Compute output: y = x * W + b\n",
    "        output = np.dot(input_data, self.weights) + self.biases\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dvalues, learning_rate=0.01):\n",
    "      \n",
    "        #Backward pass for 1x1 convolution layer\n",
    "        #dvalues shape: (batch_size, output_channels)\n",
    "       \n",
    "        batch_size = dvalues.shape[0]\n",
    "        \n",
    "        dweights = np.dot(self.input.T, dvalues) / batch_size\n",
    "        dbiases = np.sum(dvalues, axis=0) / batch_size\n",
    "        \n",
    "        dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "        self.weights -= learning_rate * dweights\n",
    "        self.biases -= learning_rate * dbiases\n",
    "        \n",
    "        return dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e492f54-527b-414e-9bdf-028b0fec547e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35, Loss: 0.0719, Accuracy: 11.17%\n",
      "Epoch 2/35, Loss: 0.0718, Accuracy: 11.24%\n",
      "Epoch 3/35, Loss: 0.0717, Accuracy: 11.25%\n",
      "Epoch 4/35, Loss: 0.0715, Accuracy: 11.35%\n",
      "Epoch 5/35, Loss: 0.0713, Accuracy: 11.81%\n",
      "Epoch 6/35, Loss: 0.0711, Accuracy: 13.44%\n",
      "Epoch 7/35, Loss: 0.0707, Accuracy: 18.96%\n",
      "Epoch 8/35, Loss: 0.0702, Accuracy: 25.30%\n",
      "Epoch 9/35, Loss: 0.0696, Accuracy: 35.42%\n",
      "Epoch 10/35, Loss: 0.0688, Accuracy: 41.27%\n",
      "Epoch 11/35, Loss: 0.0678, Accuracy: 49.15%\n",
      "Epoch 12/35, Loss: 0.0667, Accuracy: 54.40%\n",
      "Epoch 13/35, Loss: 0.0653, Accuracy: 59.30%\n",
      "Epoch 14/35, Loss: 0.0636, Accuracy: 62.22%\n",
      "Epoch 15/35, Loss: 0.0618, Accuracy: 65.17%\n",
      "Epoch 16/35, Loss: 0.0598, Accuracy: 67.70%\n",
      "Epoch 17/35, Loss: 0.0577, Accuracy: 69.43%\n",
      "Epoch 18/35, Loss: 0.0555, Accuracy: 71.10%\n",
      "Epoch 19/35, Loss: 0.0532, Accuracy: 72.82%\n",
      "Epoch 20/35, Loss: 0.0508, Accuracy: 74.01%\n",
      "Epoch 21/35, Loss: 0.0485, Accuracy: 74.97%\n",
      "Epoch 22/35, Loss: 0.0462, Accuracy: 76.15%\n",
      "Epoch 23/35, Loss: 0.0441, Accuracy: 76.96%\n",
      "Epoch 24/35, Loss: 0.0420, Accuracy: 77.89%\n",
      "Epoch 25/35, Loss: 0.0400, Accuracy: 78.49%\n",
      "Epoch 26/35, Loss: 0.0382, Accuracy: 79.26%\n",
      "Epoch 27/35, Loss: 0.0365, Accuracy: 79.87%\n",
      "Epoch 28/35, Loss: 0.0349, Accuracy: 80.36%\n",
      "Epoch 29/35, Loss: 0.0335, Accuracy: 80.83%\n",
      "Epoch 30/35, Loss: 0.0322, Accuracy: 81.26%\n",
      "Epoch 31/35, Loss: 0.0309, Accuracy: 81.60%\n",
      "Epoch 32/35, Loss: 0.0298, Accuracy: 82.02%\n",
      "Epoch 33/35, Loss: 0.0288, Accuracy: 82.30%\n",
      "Epoch 34/35, Loss: 0.0279, Accuracy: 82.59%\n",
      "Epoch 35/35, Loss: 0.0271, Accuracy: 82.91%\n",
      "\n",
      "Test Results:\n",
      "Loss: 0.8347, Accuracy: 84.21%\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype(np.float32) / 255.0\n",
    "X_test = X_test.astype(np.float32) / 255.0\n",
    "\n",
    "# Channel dimension: (N, 28, 28) -> (N, 1, 28, 28)\n",
    "X_train = X_train[:, np.newaxis, :, :]\n",
    "X_test = X_test[:, np.newaxis, :, :]\n",
    "\n",
    "y_train_onehot = one_hot_encode(y_train)\n",
    "y_test_onehot = one_hot_encode(y_test)\n",
    "\n",
    "conv_layer = ConvLayer(num_filters=2, kernel_size=3, stride=1)\n",
    "pool_layer = AvgPoolLayer(pool_size=2, stride=2)\n",
    "flatten_layer = FlattenLayer()\n",
    "conv1x1_layer = Conv1x1Layer(input_channels=2*13*13, output_channels=10)  # Size after pooling: 2x13x13\n",
    "\n",
    "epochs = 35\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    y_train_onehot_shuffled = y_train_onehot[indices]\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Get batch\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_onehot_shuffled[i:i+batch_size]\n",
    "        \n",
    "        current_batch_size = X_batch.shape[0]\n",
    "\n",
    "  # Forward pass\n",
    "        conv_out = conv_layer.forward(X_batch)\n",
    "        pool_out = pool_layer.forward(conv_out)\n",
    "        flattened = flatten_layer.forward(pool_out)\n",
    "        logits = conv1x1_layer.forward(flattened)\n",
    "        predictions = softmax(logits)\n",
    "        \n",
    "        loss = cross_entropy_loss(predictions, y_batch)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        correct_predictions += np.sum(np.argmax(predictions, axis=1) == np.argmax(y_batch, axis=1))\n",
    "        \n",
    "        # Backpropagation\n",
    "        # Gradient of softmax with cross-entropy\n",
    "        dvalues = predictions.copy()\n",
    "        dvalues[range(current_batch_size), np.argmax(y_batch, axis=1)] -= 1\n",
    "        dvalues /= current_batch_size\n",
    "        \n",
    "        # Backward pass through each layer\n",
    "        dvalues = conv1x1_layer.backward(dvalues, learning_rate)\n",
    "        dvalues = flatten_layer.backward(dvalues)\n",
    "        dvalues = pool_layer.backward(dvalues)\n",
    "        dvalues = conv_layer.backward(dvalues, learning_rate)\n",
    "    \n",
    "    accuracy = correct_predictions / len(X_train)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(X_train):.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "correct_test = 0\n",
    "test_loss = 0\n",
    "\n",
    "\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test[i:i+batch_size]\n",
    "    y_batch = y_test_onehot[i:i+batch_size]\n",
    "    \n",
    "    # Forward pass\n",
    "    conv_out = conv_layer.forward(X_batch)\n",
    "    pool_out = pool_layer.forward(conv_out)\n",
    "    flattened = flatten_layer.forward(pool_out)\n",
    "    logits = conv1x1_layer.forward(flattened)\n",
    "    predictions = softmax(logits)\n",
    "\n",
    "      \n",
    "    loss = cross_entropy_loss(predictions, y_batch)\n",
    "    test_loss += loss\n",
    "    \n",
    "\n",
    "    correct_test += np.sum(np.argmax(predictions, axis=1) == np.argmax(y_batch, axis=1))\n",
    "\n",
    "test_accuracy = correct_test / len(X_test)\n",
    "avg_test_loss = test_loss / (len(X_test) / batch_size)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Loss: {avg_test_loss:.4f}, Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cebf23b-77c9-4a23-a5dc-be6d91e4de24",
   "metadata": {},
   "source": [
    "# **Architecture**: \n",
    "The network is a simple Convolutional Neural Network (CNN) designed for digit classification on the MNIST dataset (handwritten digits 0-9). It consists of the following layers:\n",
    "\n",
    "## **Convolutional Layer (ConvLayer):**\n",
    "Parameters: 2 filters, 3x3 kernel, stride 1.\n",
    "Input: (batch_size, 1, 28, 28) (MNIST images with 1 channel).\n",
    "Output: (batch_size, 2, 26, 26) (after convolution: (28-3)//1 + 1 = 26).\n",
    "Activation: Sigmoid\n",
    "Extracts spatial features like edges from the input images\n",
    "\n",
    "## **Average Pooling Layer (AvgPoolLayer):**\n",
    "Parameters: 2x2 pool size, stride 2.\n",
    "Input: (batch_size, 2, 26, 26).\n",
    "Output: (batch_size, 2, 13, 13) (after pooling: (26-2)//2 + 1 = 13).\n",
    "Reduces spatial dimensions, making the model less sensitive to small translations.\n",
    "\n",
    "## **Flatten Layer (FlattenLayer):**\n",
    "Input: (batch_size, 2, 13, 13).\n",
    "Output: (batch_size, 2*13*13) = (batch_size, 338).\n",
    "Converts the 4D tensor into a 2D matrix for the fully connected layer.\n",
    "\n",
    "## **1x1 Convolution Layer (Conv1x1Layer):**\n",
    "Parameters: Input channels = 338, output channels = 10 (one per digit class).\n",
    "Input: (batch_size, 338).\n",
    "Output: (batch_size, 10) (logits for each class).\n",
    "Acts as a fully connected layer, mapping features to class scores.\n",
    "\n",
    "Softmax:Applied to the logits to produce a probability distribution over the 10 classes.\n",
    "\n",
    "# **Implementation:** \n",
    "The CNN is implemented from scratch using NumPy, without high-level frameworks like TensorFlow or PyTorch. Key aspects of the implementation include:\n",
    "\n",
    "## **Data Preparation:**\n",
    "MNIST dataset is loaded using keras.datasets.mnist.\n",
    "Images are normalized to [0, 1] by dividing pixel values by 255.\n",
    "Data is reshaped to (N, 1, 28, 28) to include the channel dimension.\n",
    "Labels are one-hot encoded (e.g., digit 5 â†’ [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]).\n",
    "\n",
    "**Forward Pass:** Input images pass through the ConvLayer (convolution + sigmoid), AvgPoolLayer (downsampling), FlattenLayer, and Conv1x1Layer.\n",
    "\n",
    "Softmax is applied to the final logits to get class probabilities.\n",
    "\n",
    "**Loss Function:** Cross-entropy loss is used to measure the difference between predicted probabilities and true labels, with an epsilon (1e-15) to avoid log(0).\n",
    "\n",
    "**Backward Pass:** Gradients are computed manually for each layer using backpropagation.\n",
    "The gradient of the cross-entropy loss with softmax is calculated as predictions - y_true.\n",
    "Gradients are propagated backward through the Conv1x1Layer, FlattenLayer, AvgPoolLayer, and ConvLayer, updating weights and biases with a learning rate of 0.01.\n",
    "\n",
    "## **Training:**\n",
    "The model is trained for 35 epochs with a batch size of 32.\n",
    "Data is shuffled at the start of each epoch to improve generalization.\n",
    "Loss and accuracy are computed and printed for each epoch.\n",
    "\n",
    "## **Testing:**\n",
    "The model is evaluated on the test set in batches, computing the loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d8be7a-3fe6-443a-9617-5252cf1c82aa",
   "metadata": {},
   "source": [
    "The training and test results are as follows:\n",
    "\n",
    "## **Training:**\n",
    "Over 35 epochs, the training accuracy improves from 11.17% to 82.91%.\n",
    "The training loss decreases from 0.0719 to 0.0271.\n",
    "This indicates the model learns to classify digits effectively on the training data, though the low number of filters (2) limits its capacity.\n",
    "\n",
    "## **Testing:**\n",
    "Test Accuracy: 84.21%.\n",
    "Test Loss: 0.8347.\n",
    "The test accuracy is slightly higher than the final training accuracy, but the test loss is significantly higher than the training loss (0.8347 vs. 0.0271), suggesting some overfitting. The model generalizes reasonably well but could benefit from regularization techniques like dropout or more data augmentation.\n",
    "\n",
    "The CNN architecture is minimal, with one convolutional layer (2 filters), one pooling layer, and a fully connected layer, followed by softmax for classification. The network is implemented from scratch, it achieves a test accuracy of 84.21% and a test loss of 0.8347 on MNIST. While the results are decent for such a simple model, the high test loss indicates overfitting, which could be addressed by adding more layers, using ReLU instead of sigmoid, or applying regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22007864-ea6c-45b4-bce0-f517889b0c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
