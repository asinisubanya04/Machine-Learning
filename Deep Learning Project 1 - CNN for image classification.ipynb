{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93ab2c50-5de5-492a-a62d-c2752f163868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.jit.annotations import Optional, Tuple\n",
    "from torch import Tensor\n",
    "import os\n",
    "import numpy as np\n",
    "import os.path\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.datasets as dset\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from ipywidgets import IntProgress\n",
    "import logging \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26254176-9313-4f17-bc0d-539bc6a06a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else: \n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eef3a357-9fb3-40ca-98c7-7f574b53a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "batch_train_size=64\n",
    "batch_test_size=64\n",
    "num_epochs=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fca3237-b83b-4b02-81bd-f7d77dca2236",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b43a24f-7118-4bb1-b5bb-b1d16909bbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dset.CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n",
    "test_dataset = dset.CIFAR10(root='./data', train=False, transform=test_transform, download=True)\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=batch_train_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=batch_test_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a3fbbf8-b7df-42b0-af2d-b230f47410b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_planes, kernel_1_x, kernel_3_in, kernel_3_x, kernel_5_in, kernel_5_x, pool_planes):\n",
    "        super(Inception, self).__init__()\n",
    "\n",
    "      \n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, kernel_1_x, kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_1_x),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "     \n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, kernel_3_in, kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_3_in),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(kernel_3_in, kernel_3_x, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(kernel_3_x),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "     \n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, kernel_5_in, kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_5_in),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(kernel_5_in, kernel_5_x, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(kernel_5_x),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "       \n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.b1(x)\n",
    "        out2 = self.b2(x)\n",
    "        out3 = self.b3(x)\n",
    "        out4 = self.b4(x)\n",
    "        return torch.cat([out1, out2, out3, out4], 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52b41e6d-2d3a-40b7-8bc7-3fb4a2864304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 192, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(192)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "       \n",
    "        self.inception1a = Inception(192, 64, 96, 128, 16, 32, 32)  # outputs 256 channels\n",
    "        self.inception1b = Inception(256, 128, 128, 192, 32, 96, 64)  # outputs 480 channels\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        \n",
    "        self.inception2a = Inception(480, 192, 96, 208, 16, 48, 64)  # outputs 512 channels\n",
    "        self.inception2b = Inception(512, 160, 112, 224, 24, 64, 64)  # outputs 512 channels\n",
    "        self.inception2c = Inception(512, 128, 128, 256, 24, 64, 64)  # outputs 512 channels\n",
    "        self.inception2d = Inception(512, 112, 144, 288, 32, 64, 64)  # outputs 528 channels\n",
    "        self.inception2e = Inception(528, 256, 160, 320, 32, 128, 128)  # outputs 832 channels\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "      \n",
    "        self.inception3a = Inception(832, 256, 160, 320, 32, 128, 128)  # outputs 832 channels\n",
    "        self.inception3b = Inception(832, 384, 192, 384, 48, 128, 128)  # outputs 1024 channels\n",
    "        \n",
    "    \n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=8)  # Global average pooling (8x8 feature maps)\n",
    "        self.fc = nn.Linear(1024, 10)  # Final fully connected layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initial layer\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        \n",
    " \n",
    "        x = self.inception1a(x)\n",
    "        x = self.inception1b(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "      \n",
    "        x = self.inception2a(x)\n",
    "        x = self.inception2b(x)\n",
    "        x = self.inception2c(x)\n",
    "        x = self.inception2d(x)\n",
    "        x = self.inception2e(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "      \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        \n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = CNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b67fe9c-e772-49d6-ad8c-98016c1f5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100], gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72dc129-b446-415d-a68c-5868565a6651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150]\n",
      "Train Accuracy: 53.81% | Train Loss: 983.0477\n",
      "Test Accuracy: 65.61% | Test Loss: 153.4278\n",
      "Epoch [2/150]\n",
      "Train Accuracy: 72.22% | Train Loss: 614.1575\n",
      "Test Accuracy: 70.06% | Test Loss: 129.2599\n",
      "Epoch [3/150]\n",
      "Train Accuracy: 78.05% | Train Loss: 493.5321\n",
      "Test Accuracy: 74.57% | Test Loss: 115.9805\n",
      "Epoch [4/150]\n",
      "Train Accuracy: 81.33% | Train Loss: 423.0254\n",
      "Test Accuracy: 74.75% | Test Loss: 121.4535\n",
      "Epoch [5/150]\n",
      "Train Accuracy: 83.20% | Train Loss: 377.6206\n",
      "Test Accuracy: 80.97% | Test Loss: 87.6588\n",
      "Epoch [6/150]\n",
      "Train Accuracy: 85.31% | Train Loss: 333.9218\n",
      "Test Accuracy: 80.79% | Test Loss: 88.4614\n",
      "Epoch [7/150]\n",
      "Train Accuracy: 86.68% | Train Loss: 306.5041\n",
      "Test Accuracy: 84.50% | Test Loss: 69.8143\n",
      "Epoch [8/150]\n",
      "Train Accuracy: 87.54% | Train Loss: 283.1038\n",
      "Test Accuracy: 83.35% | Test Loss: 79.9531\n",
      "Epoch [9/150]\n",
      "Train Accuracy: 88.77% | Train Loss: 258.7033\n",
      "Test Accuracy: 85.42% | Test Loss: 67.4199\n",
      "Epoch [10/150]\n",
      "Train Accuracy: 89.45% | Train Loss: 240.9136\n",
      "Test Accuracy: 81.55% | Test Loss: 87.4110\n",
      "Epoch [11/150]\n",
      "Train Accuracy: 90.07% | Train Loss: 226.0915\n",
      "Test Accuracy: 84.50% | Test Loss: 75.0662\n",
      "Epoch [12/150]\n",
      "Train Accuracy: 90.90% | Train Loss: 206.8138\n",
      "Test Accuracy: 83.40% | Test Loss: 79.7810\n",
      "Epoch [13/150]\n",
      "Train Accuracy: 91.19% | Train Loss: 198.1819\n",
      "Test Accuracy: 83.84% | Test Loss: 74.5938\n",
      "Epoch [14/150]\n",
      "Train Accuracy: 91.80% | Train Loss: 187.2799\n",
      "Test Accuracy: 85.43% | Test Loss: 68.5420\n",
      "Epoch [15/150]\n",
      "Train Accuracy: 92.36% | Train Loss: 174.8272\n",
      "Test Accuracy: 86.63% | Test Loss: 62.4889\n",
      "Epoch [16/150]\n",
      "Train Accuracy: 92.60% | Train Loss: 167.2412\n",
      "Test Accuracy: 87.33% | Test Loss: 61.5550\n",
      "Epoch [17/150]\n",
      "Train Accuracy: 92.93% | Train Loss: 157.8364\n",
      "Test Accuracy: 86.62% | Test Loss: 63.7632\n",
      "Epoch [18/150]\n",
      "Train Accuracy: 93.17% | Train Loss: 151.5278\n",
      "Test Accuracy: 86.68% | Test Loss: 68.9297\n",
      "Epoch [19/150]\n",
      "Train Accuracy: 93.56% | Train Loss: 144.1826\n",
      "Test Accuracy: 87.51% | Test Loss: 61.5280\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100. * correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100. * correct / total\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\\n\"\n",
    "          f\"Train Accuracy: {train_accuracy:.2f}% | Train Loss: {train_loss:.4f}\\n\"\n",
    "          f\"Test Accuracy: {test_accuracy:.2f}% | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'cnn_cifar10.pth')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "plt.plot(range(num_epochs), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over Epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871de49-7107-4191-b30d-ee6a1d9f14cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777cbc1-2867-428a-9b1f-d93f7d95c7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
