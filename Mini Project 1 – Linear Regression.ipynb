{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94dd83fd-e22a-4663-bf94-1eb2aa5c5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fbe27e1-9682-480f-95a1-be5477abf74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
      "[8.59304135e+00 2.32993957e+01 6.85357058e+00 2.53742935e-01\n",
      " 1.15763115e-01 7.01922514e-01 2.81210326e+01 2.10362836e+00\n",
      " 8.69865112e+00 1.68370495e+02 2.16280519e+00 9.12046075e+01\n",
      " 7.13400164e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "     warnings.filterwarnings(\"ignore\")\n",
    "     X, y = load_boston(return_X_y=True)\n",
    "# scaled X before regression\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "print(X)\n",
    "print(std)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e907c847-9f11-45f3-b020-527707487e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape #exploring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b042428-3210-49a6-9ecd-440ffd854efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebff549b-68f2-4bb4-9e4f-5cf701bb2a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of X:\n",
      " [[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00\n",
      "  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02\n",
      "  4.0300e+00]\n",
      " [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00\n",
      "  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02\n",
      "  2.9400e+00]\n",
      " [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00\n",
      "  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02\n",
      "  5.3300e+00]]\n",
      "\n",
      "First 5 values of y:\n",
      " [[24. ]\n",
      " [21.6]\n",
      " [34.7]\n",
      " [33.4]\n",
      " [36.2]]\n"
     ]
    }
   ],
   "source": [
    "y=y.reshape(-1,1) #matrix is 1D, need as vector as matrix multiplication is performed using a vector\n",
    "print(\"First 5 rows of X:\\n\", X[:5])  # features\n",
    "print(\"\\nFirst 5 values of y:\\n\", y[:5])  # target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e895571b-d7e0-4a0a-8ccb-58e62fcbc105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.]\n",
      "[[1.0000e+00 6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01\n",
      "  6.5750e+00 6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01\n",
      "  3.9690e+02 4.9800e+00]\n",
      " [1.0000e+00 2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01\n",
      "  6.4210e+00 7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01\n",
      "  3.9690e+02 9.1400e+00]]\n"
     ]
    }
   ],
   "source": [
    "X = np.c_[np.ones(X.shape[0]), X] #appending 1 to create a y intercept \n",
    "print(X[:,0])\n",
    "print(X[:2,:]) #confirming the appended 1s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf29c5ae-e7f7-4555-870a-45e16554c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training RMSE: 4.6705\n",
      "Final Testing RMSE: 4.7930\n"
     ]
    }
   ],
   "source": [
    "#linear regression \n",
    "def k_fold_cross_validation(X, y, k=10):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)  \n",
    "    rerror_train = []  # store training RMSE\n",
    "    rerror_test = []   # store testing RMSE\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # compute theta using the Normal Equation\n",
    "        theta = np.linalg.inv(X_train.T @ X_train) @ (X_train.T @ y_train)\n",
    "        \n",
    "        # compute predictions and RMSE for training set\n",
    "        y_train_pred = X_train @ theta\n",
    "        error_train = y_train - y_train_pred\n",
    "        rerror_train.append(np.sqrt((error_train.T @ error_train) / len(train_index)))\n",
    "        \n",
    "        # compute predictions and RMSE for test set\n",
    "        y_test_pred = X_test @ theta\n",
    "        error_test = y_test - y_test_pred\n",
    "        rerror_test.append(np.sqrt((error_test.T @ error_test) / len(test_index)))\n",
    "    \n",
    "    # compute average RMSE\n",
    "    rmse_train = np.mean(rerror_train)\n",
    "    rmse_test = np.mean(rerror_test)\n",
    "    \n",
    "    print(f\"Final Training RMSE: {rmse_train:.4f}\")\n",
    "    print(f\"Final Testing RMSE: {rmse_test:.4f}\")\n",
    "    \n",
    "    return rmse_train, rmse_test\n",
    "\n",
    "rmse_train, rmse_test = k_fold_cross_validation(X, y, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8368e50-eefe-4ea0-b129-b4d369ad3d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lambda: 10.0\n",
      "Best Test RMSE: 4.875913122577842\n",
      "\n",
      "Training RMSE values for different lambdas:\n",
      "Lambda 10.0: Train RMSE = 4.754308048539244\n",
      "Lambda 31.622776601683793: Train RMSE = 4.794027383267121\n",
      "Lambda 100.0: Train RMSE = 4.884360461021243\n",
      "Lambda 316.22776601683796: Train RMSE = 5.051766689258294\n",
      "Lambda 1000.0: Train RMSE = 5.2783402593600215\n",
      "Lambda 3162.2776601683795: Train RMSE = 5.601083631658911\n",
      "Lambda 10000.0: Train RMSE = 6.113324911024734\n",
      "Lambda 31622.776601683792: Train RMSE = 6.806000291864548\n",
      "Lambda 100000.0: Train RMSE = 7.351797840439606\n",
      "Lambda 316227.7660168379: Train RMSE = 7.65392416502454\n",
      "Lambda 1000000.0: Train RMSE = 7.8383882522740675\n",
      "Lambda 3162277.6601683795: Train RMSE = 7.980412523193704\n",
      "Lambda 10000000.0: Train RMSE = 8.203608836785516\n",
      "\n",
      "Testing RMSE values for different lambdas:\n",
      "Lambda 10.0: Test RMSE = 4.875913122577842\n",
      "Lambda 31.622776601683793: Test RMSE = 4.911421547108196\n",
      "Lambda 100.0: Test RMSE = 4.997655556616442\n",
      "Lambda 316.22776601683796: Test RMSE = 5.160552643761934\n",
      "Lambda 1000.0: Test RMSE = 5.37818153226861\n",
      "Lambda 3162.2776601683795: Test RMSE = 5.6860466298987\n",
      "Lambda 10000.0: Test RMSE = 6.180191582556968\n",
      "Lambda 31622.776601683792: Test RMSE = 6.856616793253613\n",
      "Lambda 100000.0: Test RMSE = 7.388909040720181\n",
      "Lambda 316227.7660168379: Test RMSE = 7.680500693157467\n",
      "Lambda 1000000.0: Test RMSE = 7.856981977691379\n",
      "Lambda 3162277.6601683795: Test RMSE = 7.9923926613294185\n",
      "Lambda 10000000.0: Test RMSE = 8.208510160468547\n"
     ]
    }
   ],
   "source": [
    "#ridge regression without regularizing bias \n",
    "\n",
    "lamda_train = {}\n",
    "lamda_test = {}\n",
    "best_lambda = None\n",
    "best_test_rmse = float('inf')  # initialize to large value to find the min\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# iterate over different lambda values\n",
    "for lamda in np.logspace(1, 7, num=13):\n",
    "    rerror_train = []\n",
    "    rerror_test = []\n",
    "    \n",
    "    for train, test in kf.split(X):  # loop over K-Folds\n",
    "        # identity matrix with bias term excluded from regularization\n",
    "        identity = np.eye(X.shape[1])\n",
    "        identity[0, 0] = 0  #no regularization of bias term\n",
    "        bias_matrix = lamda * identity\n",
    "        \n",
    "        theta = np.linalg.inv(X[train].T @ X[train] + bias_matrix) @ (X[train].T @ y[train])\n",
    "        \n",
    "        y_train_pred = X[train] @ theta\n",
    "        error_train = y[train] - y_train_pred\n",
    "        rerror_train.append(np.sqrt((error_train.T @ error_train) / len(train)))\n",
    "        \n",
    "        y_test_pred = X[test] @ theta\n",
    "        error_test = y[test] - y_test_pred\n",
    "        rerror_test.append(np.sqrt((error_test.T @ error_test) / len(test)))\n",
    "    \n",
    "    # average RMSE for the current lambda\n",
    "    rmse_train = np.mean(rerror_train)\n",
    "    rmse_test = np.mean(rerror_test)\n",
    "    \n",
    "    # store RMSE values for each lambda\n",
    "    lamda_train[str(lamda)] = rmse_train\n",
    "    lamda_test[str(lamda)] = rmse_test\n",
    "    \n",
    "    # to get lowest test RMSE\n",
    "    if rmse_test < best_test_rmse:\n",
    "        best_test_rmse = rmse_test\n",
    "        best_lambda = lamda\n",
    "\n",
    "print(\"Best Lambda:\", best_lambda)\n",
    "print(\"Best Test RMSE:\", best_test_rmse)\n",
    "print(\"\\nTraining RMSE values for different lambdas:\")\n",
    "for lamda, rmse in lamda_train.items():\n",
    "    print(f\"Lambda {lamda}: Train RMSE = {rmse}\")\n",
    "    \n",
    "print(\"\\nTesting RMSE values for different lambdas:\")\n",
    "for lamda, rmse in lamda_test.items():\n",
    "    print(f\"Lambda {lamda}: Test RMSE = {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efd36897-64a0-4ae3-9d53-f372a57cadf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Best Lambda = 10.0:\n",
      "Average Training RMSE: 4.754308048539244\n",
      "Average Testing RMSE: 4.875913122577842\n"
     ]
    }
   ],
   "source": [
    "#Performance of best regularization parameter\n",
    "\n",
    "best_lambda = best_lambda  #from previous loop\n",
    "\n",
    "#variables initalized to track the performance for this best lambda\n",
    "rerror_train_best = []\n",
    "rerror_test_best = []\n",
    "\n",
    "#K-Fold cross-validation rerun with the best lambda\n",
    "for train, test in kf.split(X):\n",
    "    identity = np.eye(X.shape[1])\n",
    "    identity[0, 0] = 0 \n",
    "    bias_matrix = best_lambda * identity\n",
    "    \n",
    "    theta = np.linalg.inv(X[train].T @ X[train] + bias_matrix) @ (X[train].T @ y[train])\n",
    "    \n",
    "    y_train_pred = X[train] @ theta\n",
    "    error_train = y[train] - y_train_pred\n",
    "    rerror_train_best.append(np.sqrt((error_train.T @ error_train) / len(train)))\n",
    "    \n",
    "    y_test_pred = X[test] @ theta\n",
    "    error_test = y[test] - y_test_pred\n",
    "    rerror_test_best.append(np.sqrt((error_test.T @ error_test) / len(test)))\n",
    "\n",
    "avg_rmse_train_best = np.mean(rerror_train_best)\n",
    "avg_rmse_test_best = np.mean(rerror_test_best)\n",
    "\n",
    "print(f\"\\nFor Best Lambda = {best_lambda}:\")\n",
    "print(f\"Average Training RMSE: {avg_rmse_train_best}\")\n",
    "print(f\"Average Testing RMSE: {avg_rmse_test_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3808f799-af3c-4c10-a08c-bb8a844e561d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error for train data:  2.5929733674111586\n",
      "Root mean squared error for test data:  3.6514250736912266\n"
     ]
    }
   ],
   "source": [
    "#Polynomial regression \n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_transformed = poly.fit_transform(X)\n",
    "\n",
    "rerror_train = []\n",
    "rerror_test = []\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for train, test in kf.split(X_transformed):\n",
    "    \n",
    "    identity = np.eye(X_transformed.shape[1]) \n",
    "    identity[0, 0] = 0 \n",
    "    bias_matrix = best_lambda * identity\n",
    "    \n",
    "    theta = np.dot(np.linalg.inv(np.dot(X_transformed[train].T, X_transformed[train]) + bias_matrix), np.dot(X_transformed[train].T, y[train]))\n",
    "    \n",
    "    # Training error\n",
    "    y_hyp = np.dot(X_transformed[train], theta)\n",
    "    error = y[train] - y_hyp\n",
    "    rerror_train.append(np.sqrt(np.dot(error.T, error) / len(train)))\n",
    "    \n",
    "    # Test error\n",
    "    y_hyp = np.dot(X_transformed[test], theta)\n",
    "    error = y[test] - y_hyp\n",
    "    rerror_test.append(np.sqrt(np.dot(error.T, error) / len(test)))\n",
    "\n",
    "rmse_train = np.mean(rerror_train)\n",
    "rmse_test = np.mean(rerror_test)\n",
    "\n",
    "print(\"Root mean squared error for train data: \", rmse_train)\n",
    "print(\"Root mean squared error for test data: \", rmse_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d897bf9b-0a91-43fe-9c80-2feca44f062f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2388240/2230043326.py:20: RuntimeWarning: invalid value encountered in subtract\n",
      "  theta = theta - (learning_rate*gradients)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error for train data:  nan\n",
      "Root mean squared error for test data:  nan\n"
     ]
    }
   ],
   "source": [
    "#Multi-variate linear regression using gradient descent\n",
    "\n",
    "learning_rate = 0.0001\n",
    "n_iterations = 1000\n",
    "m=506\n",
    "\n",
    "rerror_train = []\n",
    "rerror_test = []\n",
    "\n",
    "theta = np.random.randn(X.shape[1],1)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for train,test in kf.split(X):\n",
    "    for iteration in range(n_iterations):\n",
    "        gradients = 2/len(train) * (X[train].T.dot(X[train].dot(theta)-y[train]))\n",
    "        theta = theta - (learning_rate*gradients)\n",
    "    y_hyp = np.dot(X[train],theta)\n",
    "    error = np.array(y[train]-y_hyp)\n",
    "    rerror_train.append(np.sqrt((np.dot(error.T,error))/len(train)))\n",
    "    y_hyp = np.dot(X[test],theta)\n",
    "    error = np.array(y[test]-y_hyp)\n",
    "    rerror_test.append(np.sqrt((np.dot(error.T,error))/len(test)))\n",
    "\n",
    "rmse_train = np.mean(rerror_train)\n",
    "rmse_test = np.mean(rerror_test)\n",
    "print(\"Root mean squared error for train data: \",rmse_train)\n",
    "print(\"Root mean squared error for test data: \",rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4df6e68-d5d3-4fb2-9d99-32ef7c150c48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (14,1) (13,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m gradients \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(theta)\n\u001b[1;32m     22\u001b[0m gradients[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train)) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(errors)\n\u001b[0;32m---> 23\u001b[0m gradients[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m theta \u001b[38;5;241m=\u001b[39m theta \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m gradients\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misnan(gradients)) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misnan(theta)):\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (14,1) (13,1) "
     ]
    }
   ],
   "source": [
    "#Lasso Regression\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_iterations = 1000\n",
    "alpha = 0.001\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "rerror_train = []\n",
    "rerror_test = []\n",
    "\n",
    "theta = np.random.randn(X.shape[1], 1)  \n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        predictions = X_train.dot(theta)\n",
    "        errors = predictions - y_train\n",
    "        gradients = np.zeros_like(theta)\n",
    "        gradients[0] = (2 / len(X_train)) * np.sum(errors)\n",
    "        gradients[1:] = (2 / len(X_train)) * X_train.T.dot(errors) + alpha * np.sign(theta[1:])\n",
    "        \n",
    "        theta = theta - learning_rate * gradients\n",
    "        \n",
    "        if np.any(np.isnan(gradients)) or np.any(np.isnan(theta)):\n",
    "            print(f\"NaN or Inf detected in gradients or theta at iteration {iteration}\")\n",
    "            break\n",
    "\n",
    "    y_train_pred = X_train.dot(theta)\n",
    "    error_train = y_train - y_train_pred\n",
    "    rerror_train.append(np.sqrt(np.mean(error_train**2)))\n",
    "\n",
    "    y_test_pred = X_test.dot(theta)\n",
    "    error_test = y_test - y_test_pred\n",
    "    rerror_test.append(np.sqrt(np.mean(error_test**2)))\n",
    "\n",
    "rmse_train = np.mean(rerror_train)\n",
    "rmse_test = np.mean(rerror_test)\n",
    "\n",
    "print(f\"Root mean squared error for train data: {rmse_train}\")\n",
    "print(f\"Root mean squared error for test data: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a88f7979-d513-4c87-834e-37c93eaaffbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (14,1) (13,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(theta)\n\u001b[1;32m     25\u001b[0m     gradients[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train)) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(errors)\n\u001b[0;32m---> 27\u001b[0m     gradients[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     theta \u001b[38;5;241m=\u001b[39m theta \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m gradients\n\u001b[1;32m     32\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mdot(theta)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (14,1) (13,1) "
     ]
    }
   ],
   "source": [
    "#Elastic Net\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_iterations = 1000\n",
    "alpha = 0.001  \n",
    "r = 0.5 \n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "rerror_train = []\n",
    "rerror_test = []\n",
    "\n",
    "theta = np.random.randn(X.shape[1], 1) \n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        predictions = X_train.dot(theta)\n",
    "        errors = predictions - y_train\n",
    "        \n",
    "        gradients = np.zeros_like(theta)\n",
    "        \n",
    "        gradients[0] = (2 / len(X_train)) * np.sum(errors)\n",
    "        \n",
    "        gradients[1:] = (2 / len(X_train)) * X_train.T.dot(errors) + alpha * (r * np.sign(theta[1:]) + (1 - r) * 2 * theta[1:])\n",
    "        \n",
    "        theta = theta - learning_rate * gradients\n",
    "        \n",
    "    \n",
    "    y_train_pred = X_train.dot(theta)\n",
    "    error_train = y_train - y_train_pred\n",
    "    rerror_train.append(np.sqrt(np.mean(error_train**2)))\n",
    "\n",
    "    y_test_pred = X_test.dot(theta)\n",
    "    error_test = y_test - y_test_pred\n",
    "    rerror_test.append(np.sqrt(np.mean(error_test**2)))\n",
    "\n",
    "rmse_train = np.mean(rerror_train)\n",
    "rmse_test = np.mean(rerror_test)\n",
    "\n",
    "print(f\"Root mean squared error for train data: {rmse_train}\")\n",
    "print(f\"Root mean squared error for test data: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199dde85-a668-45cc-bbf6-d6062500c983",
   "metadata": {},
   "source": [
    "Most probably a model that was optimized using gradient descent with the smallest rmse for all the test data amongst all the models would be the best choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8910d-860f-4460-8f67-44ce6a216f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
